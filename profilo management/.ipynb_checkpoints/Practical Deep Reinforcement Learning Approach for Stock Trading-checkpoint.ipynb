{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy8r7_g5WjAT"
   },
   "source": [
    "# Practical Deep Reinforcement Learning Approach for Stock Trading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uH1KXctgnoJ"
   },
   "source": [
    "# Section 1 Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "a2oD49e_N_05"
   },
   "outputs": [],
   "source": [
    "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "j37flV31OJGW"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'finrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qy/5qdw_s4x2_xfc5b6bthxnb200000gn/T/ipykernel_34334/3289777177.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myfinance\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0myf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myahoodownloader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mYahooDownloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureEngineer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_tickers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'finrl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "\n",
    "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
    "from finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
    "from finrl import config_tickers\n",
    "from finrl.config import INDICATORS\n",
    "\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wxsN8i7tg07U"
   },
   "source": [
    "# Section 2 Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMNm9tCMXy8J"
   },
   "source": [
    "## Section 2.1 Data Downloader\n",
    "\n",
    "[yfinance](https://github.com/ranaroussi/yfinance) is an open-source library that provides APIs fetching historical data form Yahoo Finance. In FinRL, we have a class called [YahooDownloader](https://github.com/AI4Finance-Foundation/FinRL/blob/master/finrl/meta/preprocessor/yahoodownloader.py) that use yfinance to fetch data from Yahoo Finance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWVXUkzaZE8m"
   },
   "source": [
    "**OHLCV**: Data downloaded are in the form of OHLCV, corresponding to **open, high, low, close, volume,** respectively. OHLCV is important because they contain most of numerical information of a stock in time series. From OHLCV, traders can get further judgement and prediction like the momentum, people's interest, market trends, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRYlbdMpW9Np"
   },
   "source": [
    "### Data for a single ticker (can skip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wo6pCQYXDbz"
   },
   "source": [
    "Here we provide two ways to fetch data with single ticker, let's take Apple Inc. (AAPL) as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzVRe90WXLB1"
   },
   "source": [
    "#### Using yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SSl6mVV7XNw6",
    "outputId": "460c06eb-c71d-4ebb-fe17-481295d70cff"
   },
   "outputs": [],
   "source": [
    "aapl_df_yf = yf.download(tickers = \"aapl\", start='2020-01-01', end='2020-01-31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "Rjutz22rXrpR",
    "outputId": "62aadc8c-b854-403d-ac73-6cf86d53fa22"
   },
   "outputs": [],
   "source": [
    "aapl_df_yf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHZLDmnsXOK0"
   },
   "source": [
    "#### Using FinRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFB77ohNbXCc"
   },
   "source": [
    "In FinRL's YahooDownloader, we modified the data frame to the form that convenient for further data processing process. We use adjusted close price instead of close price, and add a column representing the day of a week (0-4 corresponding to Monday-Friday)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7ufDHvt4XBWT",
    "outputId": "41603042-4f14-4814-c569-305d85fa7f9d"
   },
   "outputs": [],
   "source": [
    "aapl_df_finrl = YahooDownloader(start_date = '2020-01-01',\n",
    "                                end_date = '2020-01-31',\n",
    "                                ticker_list = ['aapl']).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_TgEjXxhXtT_",
    "outputId": "a8e8a9e2-a1ea-472e-eddf-2227e6c901d8"
   },
   "outputs": [],
   "source": [
    "aapl_df_finrl.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9kcOE5nbic6R"
   },
   "source": [
    "### Data for the chosen tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xTPG4Fhc-zL"
   },
   "outputs": [],
   "source": [
    "TRAIN_START_DATE = '2009-01-01'\n",
    "TRAIN_END_DATE = '2015-01-01'\n",
    "TRADE_START_DATE = '2016-01-01'\n",
    "TRADE_END_DATE = '2018-01-01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9LblMI8CO0F3",
    "outputId": "7be76385-50eb-4e8d-f2e5-1795d77b70ba"
   },
   "outputs": [],
   "source": [
    "df_raw = YahooDownloader(start_date = TRAIN_START_DATE,\n",
    "                     end_date = TRADE_END_DATE,\n",
    "                     ticker_list = config_tickers.DOW_30_TICKER).fetch_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "RD9cjHzt8X3A",
    "outputId": "051acda5-c8fd-440a-a5af-6be04cfdc018"
   },
   "outputs": [],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqC6c40Zh1iH"
   },
   "source": [
    "## Section 2.2 Preprocess Data\n",
    "We need to check for missing data and do feature engineering to convert the data point into a state.\n",
    "\n",
    "* **Adding technical indicators**. In practical trading, various information needs to be taken into account, such as historical prices, current holding shares, technical indicators, etc. Here, we demonstrate two trend-following technical indicators: MACD and RSI. Moving average convergence/divergence (MACD) is one of the most commonly used indicator showing bull and bear market. Its calculation is based on EMA (Exponential Moving Average indicator, measuring trend direction over a period of time.)\n",
    "\n",
    "* **Adding turbulence index**. Risk-aversion reflects whether an investor prefers to protect the capital. It also influences one's trading strategy when facing different market volatility level. To control the risk in a worst-case scenario, such as financial crisis of 2007–2008, we may consider the turbulence index that measures extreme fluctuation of asset price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PmKP-1ii3RLS",
    "outputId": "d514cf1a-8609-402e-ad58-df5f9100ec85"
   },
   "outputs": [],
   "source": [
    "fe = FeatureEngineer(use_technical_indicator=True,\n",
    "                     tech_indicator_list = INDICATORS,\n",
    "                     use_vix=True,\n",
    "                     use_turbulence=True,\n",
    "                     user_defined_feature = False)\n",
    "\n",
    "processed = fe.preprocess_data(df_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kixon2tR3RLT"
   },
   "outputs": [],
   "source": [
    "list_ticker = processed[\"tic\"].unique().tolist()\n",
    "list_date = list(pd.date_range(processed['date'].min(),processed['date'].max()).astype(str))\n",
    "combination = list(itertools.product(list_date,list_ticker))\n",
    "\n",
    "processed_full = pd.DataFrame(combination,columns=[\"date\",\"tic\"]).merge(processed,on=[\"date\",\"tic\"],how=\"left\")\n",
    "processed_full = processed_full[processed_full['date'].isin(processed['date'])]\n",
    "processed_full = processed_full.sort_values(['date','tic'])\n",
    "\n",
    "processed_full = processed_full.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "id": "HwKJNWJSabNK",
    "outputId": "16c8080e-91b0-4e8d-9a09-44939ac69801"
   },
   "outputs": [],
   "source": [
    "processed_full.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydLNxwdPIJhW"
   },
   "source": [
    "## Section 2.3 Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iEiXDdUhZP7R",
    "outputId": "554b1c09-6d6f-48fb-c724-351b40a2ddaf"
   },
   "outputs": [],
   "source": [
    "# Split the data for training and trading\n",
    "train = data_split(processed_full, TRAIN_START_DATE,TRAIN_END_DATE)\n",
    "trade = data_split(processed_full, TRADE_START_DATE,TRADE_END_DATE)\n",
    "# Save data to csv file\n",
    "train.to_csv('train_data.csv')\n",
    "trade.to_csv('trade_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. \n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal.\n",
    "\n",
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1 Build the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 10000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)\n",
    "env_train, _ = e_train_gym.get_sb_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2 Train RL agent \n",
    "RL agents are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. We use **[SAC](https://arxiv.org/abs/1801.01290)** as an example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent = DRLAgent(env = env_train)\n",
    "# SAC_PARAMS = {\n",
    "#     \"batch_size\": 128,\n",
    "#     \"buffer_size\": 100000,\n",
    "#     \"learning_rate\": 0.0001,\n",
    "#     \"learning_starts\": 100,\n",
    "#     \"ent_coef\": \"auto_0.1\",\n",
    "# }\n",
    "\n",
    "# model_sac = agent.get_model(\"sac\", model_kwargs = SAC_PARAMS)\n",
    "# # set up logger\n",
    "# tmp_path = RESULTS_DIR + '/sac'\n",
    "# new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_sac.set_logger(new_logger_sac)\n",
    "\n",
    "# trained_sac = agent.train_model(model=model_sac, \n",
    "#                              tb_log_name='sac',\n",
    "#                              total_timesteps=100000)\n",
    "# # save model\n",
    "# trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") \n",
    "\n",
    "# you may also use other RL algorithms such as A2C, DDPG, PPO, TD3\n",
    "\n",
    "# # A2C https://arxiv.org/abs/1602.01783\n",
    "# agent = DRLAgent(env = env_train)\n",
    "# model_a2c = agent.get_model(\"a2c\")\n",
    "# # set up logger\n",
    "# tmp_path = RESULTS_DIR + '/a2c'\n",
    "# new_logger_a2c = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_a2c.set_logger(new_logger_a2c)\n",
    "# trained_a2c = agent.train_model(model=model_a2c, \n",
    "#                              tb_log_name='a2c',\n",
    "#                              total_timesteps=50000) \n",
    "# trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\")\n",
    "\n",
    "# DDPG https://arxiv.org/abs/1509.02971\n",
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model(\"ddpg\")\n",
    "# set up logger\n",
    "tmp_path = RESULTS_DIR + '/ddpg'\n",
    "new_logger_ddpg = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# Set new logger\n",
    "model_ddpg.set_logger(new_logger_ddpg)\n",
    "trained_ddpg = agent.train_model(model=model_ddpg, \n",
    "                             tb_log_name='ddpg',\n",
    "                             total_timesteps=50000)\n",
    "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\")\n",
    "\n",
    "# # PPO https://arxiv.org/abs/1707.06347\n",
    "# agent = DRLAgent(env = env_train)\n",
    "# PPO_PARAMS = {\n",
    "#     \"n_steps\": 2048,\n",
    "#     \"ent_coef\": 0.01,\n",
    "#     \"learning_rate\": 0.00025,\n",
    "#     \"batch_size\": 128,\n",
    "# }\n",
    "# model_ppo = agent.get_model(\"ppo\",model_kwargs = PPO_PARAMS)\n",
    "# # set up logger\n",
    "# tmp_path = RESULTS_DIR + '/ppo'\n",
    "# new_logger_ppo = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_ppo.set_logger(new_logger_ppo)\n",
    "# trained_ppo = agent.train_model(model=model_ppo, \n",
    "#                              tb_log_name='ppo',\n",
    "#                              total_timesteps=200000) \n",
    "# trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") \n",
    "\n",
    "# # TD3 https://arxiv.org/pdf/1802.09477\n",
    "# agent = DRLAgent(env = env_train)\n",
    "# TD3_PARAMS = {\"batch_size\": 100, \n",
    "#               \"buffer_size\": 1000000, \n",
    "#               \"learning_rate\": 0.001}\n",
    "\n",
    "# model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "# # set up logger\n",
    "# tmp_path = RESULTS_DIR + '/td3'\n",
    "# new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "# # Set new logger\n",
    "# model_td3.set_logger(new_logger_td3)\n",
    "# trained_td3 = agent.train_model(model=model_td3, \n",
    "#                              tb_log_name='td3',\n",
    "#                              total_timesteps=50000) \n",
    "# trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 Backtesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3 import SAC, A2C, DDPG, PPO, TD3\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "trade = pd.read_csv('trade_data.csv')\n",
    "\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "trade = trade.set_index(trade.columns[0])\n",
    "trade.index.names = ['']\n",
    "\n",
    "trained_sac = SAC.load(TRAINED_MODEL_DIR + \"/agent_sac\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.1 RL (Out-of-sample performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_trade_gym = StockTradingEnv(df = trade, risk_indicator_col='vix', **env_kwargs)\n",
    "df_account_value_sac, df_actions_sac = DRLAgent.DRL_prediction(\n",
    "    model=trained_sac, \n",
    "    environment = e_trade_gym)\n",
    "df_account_value_ddpg, df_actions_ddpg = DRLAgent.DRL_prediction(\n",
    "    model=trained_ddpg, \n",
    "    environment = e_trade_gym)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.2 Mean-variance optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "# Some helper functions\n",
    "def process_df_for_mvo(df):\n",
    "  return df.pivot(index=\"date\", columns=\"tic\", values=\"close\")\n",
    "\n",
    "def StockReturnsComputing(StockPrice, Rows, Columns): \n",
    "  StockReturn = np.zeros([Rows-1, Columns]) \n",
    "  for j in range(Columns):        # j: Assets \n",
    "    for i in range(Rows-1):     # i: Daily Prices \n",
    "      StockReturn[i,j]=((StockPrice[i+1, j]-StockPrice[i,j])/StockPrice[i,j])* 100 \n",
    "      \n",
    "  return StockReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StockData = process_df_for_mvo(train)\n",
    "TradeData = process_df_for_mvo(trade)\n",
    "\n",
    "# compute asset returns\n",
    "arStockPrices = np.asarray(StockData)\n",
    "[Rows, Cols]=arStockPrices.shape\n",
    "arReturns = StockReturnsComputing(arStockPrices, Rows, Cols)\n",
    "\n",
    "# compute mean returns and variance covariance matrix of returns\n",
    "meanReturns = np.mean(arReturns, axis = 0)\n",
    "covReturns = np.cov(arReturns, rowvar=False)\n",
    " \n",
    "# set precision for printing results\n",
    "np.set_printoptions(precision=3, suppress = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ef_mean = EfficientFrontier(meanReturns, covReturns, weight_bounds=(0, 0.1))\n",
    "raw_weights_mean = ef_mean.max_sharpe()\n",
    "cleaned_weights_mean = ef_mean.clean_weights()\n",
    "mvo_weights = np.array([10000 * cleaned_weights_mean[i] for i in range(len(cleaned_weights_mean))])\n",
    "\n",
    "LastPrice = np.array([1/p for p in StockData.tail(1).to_numpy()[0]])\n",
    "Initial_Portfolio = np.multiply(mvo_weights, LastPrice)\n",
    "\n",
    "Portfolio_Assets = TradeData @ Initial_Portfolio\n",
    "MVO_result = pd.DataFrame(Portfolio_Assets, columns=[\"Mean Var\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.3 DJIA index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dji = YahooDownloader(\n",
    "    start_date=TRADE_START_DATE, end_date=TRADE_END_DATE, ticker_list=[\"dji\"]\n",
    ").fetch_data()\n",
    "df_dji = df_dji[[\"date\", \"close\"]]\n",
    "fst_day = df_dji[\"close\"][0]\n",
    "dji = pd.merge(\n",
    "    df_dji[\"date\"],\n",
    "    df_dji[\"close\"].div(fst_day).mul(10000),\n",
    "    how=\"outer\",\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ").set_index(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4.4 Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result_sac = (\n",
    "    df_account_value_sac.set_index(df_account_value_sac.columns[0])\n",
    ")\n",
    "df_result_ddpg = (\n",
    "    df_account_value_ddpg.set_index(df_account_value_ddpg.columns[0])\n",
    ")\n",
    "result = pd.DataFrame(\n",
    "    {\n",
    "        \"ddpg\": df_result_ddpg[\"account_value\"],\n",
    "        \"sac\": df_result_sac[\"account_value\"],\n",
    "        \"mvo\": MVO_result[\"Mean Var\"],\n",
    "        \"dji\": dji[\"close\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.figure()\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
